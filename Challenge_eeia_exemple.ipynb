{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge EEIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_time_features(df_raw: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df_raw.copy()\n",
    "    df[\"Month\"] = df.index.map(lambda datetime: datetime.month)\n",
    "    df[\"Day\"] = df.index.map(lambda datetime: datetime.day)\n",
    "    df[\"Hour\"] = df.index.map(lambda datetime: datetime.hour)\n",
    "    df[\"Minute\"] = df.index.map(lambda datetime: datetime.minute)\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define the directory path where the data is stored and will be stored. The files train.csv and test.csv should be in that directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_raw = pd.read_csv(f'{data_path}train.csv',delimiter=';',decimal=',',na_values=[\"#VALEUR!\"],index_col=\"time\")\n",
    "data_raw.index = pd.to_datetime(data_raw.index, format='%d/%m/%Y %H:%M')\n",
    "data_raw.sort_index(inplace=True)\n",
    "data = extract_time_features(data_raw)\n",
    "\n",
    "test_raw = pd.read_csv(f'{data_path}test.csv',delimiter=';',decimal=',',na_values=[\"#VALEUR!\"],index_col=\"time\")\n",
    "test_raw.index = pd.to_datetime(test_raw.index, format='%d/%m/%Y %H:%M')\n",
    "test_raw.sort_index(inplace=True)\n",
    "test = extract_time_features(test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell displays the first rows of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the shape of the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape, test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set contains 368591 rows andd 12 columns and the test set contains 156960 rows and 12 columns. The test set contains almost half less data than the training set. All the values in the column Net Power (MW) are NaN and should be filled with our model's predictions.\n",
    "\n",
    "Here are the list of the columns of both dataset : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"0\" padding=\"0\" margin=\"0\">\n",
    "<tbody>\n",
    "<tr><td>Amb temp (°C)</td><td>Température ambiante</td></tr>\n",
    "<tr><td>Comp inlet temp (°C)</td><td>Température entrée compresseur</td></tr>\n",
    "<tr><td>amb pressure</td><td>Pression ambiante</td></tr>\n",
    "<tr><td>HR %</td><td>%Humidité relative ambiante</td></tr>\n",
    "<tr><td>C/H</td><td>Rapport Carbonne/hydrogène du Gaz Nat</td></tr>\n",
    "<tr><td>Network Frequency (Hz)</td><td>Fréquence du réseau électrique en Hz</td></tr>\n",
    "<tr><td>Lower Heating Value (Wh/Nm3) &nbsp;</td><td>le pouvoir calorifique inférieur du Gaz Nat</td></tr>\n",
    "<tr><td>EOH (h)</td><td>Heures d'Opérations Equivalentes </td></tr>\n",
    "<tr><td>DP filtre</td><td>Perte de charge au niveau des filtres d'air entrée turbine à gaz</td></tr>\n",
    "<tr><td>CTRL anti givrage</td><td>Control de la vanne d'ouverture de l'anti-givre entrée turbine gaz</td></tr>\n",
    "<tr><td>IGV %</td><td>% d'ouverture de la valve IGV (Inlet Guide Vanes) afin de controller la charge de la CCGT</td></tr>\n",
    "<tr><td>Net Power (MW)</td><td>Production d'électricité NETTE générée par la CCGT</td></tr>\n",
    "</tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check that this data has the exact types we are waiting for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert data.dtypes.equals(pd.Series(dict(zip(data.columns,[\"float64\"]*len(data.columns)))))\n",
    "\n",
    "# assert test.dtypes.equals(pd.Series(dict(zip(test.columns,[\"float64\"]*len(test.columns)))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Statistical metrics of the training set such as the minimum, average, standard deviation, maximum, and quantiles are computed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and first models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 3388 NaN values in the dataframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping all the NaN values\n",
    "# data.dropna(inplace=True)\n",
    "\n",
    "# Replace the Nans using a given strategy\n",
    "\n",
    "chosen_strategy = 'median' # \"mean\" / \"constant\" / \"most_frequent\"\n",
    "for col in data.columns:\n",
    "    imp_mean = SimpleImputer(missing_values=np.nan, strategy='median')\n",
    "    data[col] = imp_mean.fit_transform(data[[col]]).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (data.isna().sum(axis=0).sum() == 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by checking all the variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,16),nrows=3, ncols=4)\n",
    "\n",
    "date_hour = pd.to_datetime(\"2022-01-01 00:00:00\")\n",
    "for i,col in enumerate(data.columns[:12]):\n",
    "    data.loc[date_hour: date_hour + pd.Timedelta(\"1d\"), col].plot(ax=ax[i//4,i%4],title=col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.hist(figsize=(20,16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Net Power (MW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"Net Power (MW)\"].plot(figsize=(20,12))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if there is any correlation between the Net power (MW) and speed by taking a look at one of them, function of the other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_var = \"Amb temp °C\"\n",
    "y_var = \"Net Power (MW)\"\n",
    "# data.plot.scatter(x=x_var,y=y_var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,16),nrows=3, ncols=4)\n",
    "\n",
    "for i,col in enumerate(data.columns[:12]):\n",
    "    data.plot.scatter(x=col,y=y_var,ax=ax[i//4,i%4],title=col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The correlation between the variables is not obvious expect for the variables 'DP filtre' and 'IGV %'. Let's take a closer look at the values of the correlations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.corr().style.background_gradient()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'DP filtre' and 'IGV %' are highly correlated with the target variable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a sample, the correlation is defined by : \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\frac{\\sum \\limits _{i=1} ^{n} (x_{i} - \\bar x) (y_{i} - \\bar y)}{\\sqrt{\\sum \\limits _{i=1} ^{n}(x_{i} - \\bar x)^{2}}\\sqrt{\\sum \\limits _{i=1} ^{n}(y_{i} - \\bar y)^{2}}} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is important to recall is that it is comprised in the range $[-1, 1]$ and : <br>\n",
    "    - it is equal to 1 if the two variables are exactly the same <br>\n",
    "    - it is equal to -1 if the two variables are the exact opposite <br>\n",
    "    - when it is equal to 0, the two variables have nothing in common : they are independent one from the other, for example this could be the value of the bitcoin and the average wind speed in south korea, we know these two have nothing in common.<br>\n",
    "    - when it is > 0, the two variables are positively correlated, this means that on average, when one goes up, the other goes up too.<br>\n",
    "    - when it is < 0, the two variables are negatively correlated, this means that on average, when one goes up, the other goes down.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EDA ideas\n",
    "\n",
    "- features selection: drop useless variables? why? how?\n",
    "- handle the nan values differently\n",
    "- create new variables ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this first EDA, a very simple model we can try to predict our sample is to try a linear model : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to do so, we import some libraries that will be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now cut our dataframe in two : one dataframe will be used for training, and the other one will be used to estimate what is the value of this first model we have made. \n",
    "For this, why do we not directly use the test set ? The reason is that for the test set, we do not know the exact value of the power measurement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(columns=\"Net Power (MW)\")\n",
    "y = data[\"Net Power (MW)\"]\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_eval, y_train, y_eval = train_test_split(X, y,test_size=0.2, shuffle=True,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model):\n",
    "    print(\"Model name: \", type(model).__name__)\n",
    "    print(\"Model parameters: \", model.get_params())\n",
    "\n",
    "    # Printing model accuracy\n",
    "    model_train_mae = mean_absolute_error(y_train,model.predict(X_train))\n",
    "    model_test_mae = mean_absolute_error(y_eval,model.predict(X_eval))\n",
    "\n",
    "    print(\"Model Mean Absolute error on the train set : %.2f\" % model_train_mae)\n",
    "    print(\"Model Mean Absolute error on the test set : %.2f\" % model_test_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "evaluate_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nice ! We have our first model and it gives an error of 10.27 !..\n",
    "\n",
    "Now wait, what is the value of that first model ? How can we know if 10.27 is actually a good error ? Well for this, a very neat way to be able to know if our model is worth anything is to compare it to a naive model. A naive model can be for example to predict everytime the same value, whatever the conditions. One of these naive model we have at hand would be to predict the mean value of the wind power in the train set. Let's see what would this model give. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = DummyRegressor(strategy='mean')\n",
    "dummy_model.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(dummy_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_model = DummyRegressor(strategy='median')\n",
    "dummy_model.fit(X_train, y_train)\n",
    "\n",
    "evaluate_model(dummy_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes ! Good news, our model did really learn something good ! We are a lot better than the 'mean' or 'median' model, around 10 times better, based on this metric."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have already seen first models above : the linear model with all the variables , and two naive models (median, and mean). It will be your job from now on to determine the best model, but let's already take a look at one classic model that data scientists usually try on for nearly any subject : Random Forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestRegressor(\n",
    "    n_jobs=-1,\n",
    "    max_depth=10,\n",
    "    # max_samples=200,\n",
    "    n_estimators=100,\n",
    "    # criterion=\"absolute_error\"\n",
    ")\n",
    "rf_model.fit(X_train, y_train)\n",
    "evaluate_model(rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forest does better than the linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score, KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_rf_model = RandomForestRegressor(n_jobs=-1,max_samples=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_scores = []\n",
    "# for i in range(2, 11):\n",
    "#     kfold = KFold(n_splits=i)\n",
    "#     scores = cross_val_score(cv_rf_model, X, y,scoring=\"neg_mean_absolute_error\", cv=kfold)\n",
    "#     cv_scores.append(-scores.mean())\n",
    "\n",
    "# best_cv = cv_scores.index(max(cv_scores)) + 2\n",
    "# print(f\"The best cross-validation split is {best_cv}-fold with a mean score of {max(cv_scores)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train, X_eval, y_train, y_eval = train_test_split(X, y,test_size=1/best_cv, shuffle=True,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_rf_model.fit(X_train, y_train)\n",
    "# evaluate_model(cv_rf_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling ideas\n",
    "\n",
    "- Try different parameters for the models ?\n",
    "- Try new models ?\n",
    "- Tune parameters ?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predictions on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now our model is fit, we can pass on to the predictions.\n",
    "\n",
    "_Note: be careful when generating your submission file. Indeed, it needs to be a csv file with \";\" as separator._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_model = rf_model #cv_rf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test.drop(columns=\"Net Power (MW)\")\n",
    "\n",
    "df_predictions = pd.DataFrame({\n",
    "    'time': test.index,\n",
    "    'Net Power (MW)': selected_model.predict(X_test),\n",
    "})\n",
    "\n",
    "df_predictions.to_csv('data/predictions.csv', date_format='%d/%m/%Y %H:%M',index=False, sep=';')\n",
    "df_predictions.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is your turn, what better model can you think of ?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
